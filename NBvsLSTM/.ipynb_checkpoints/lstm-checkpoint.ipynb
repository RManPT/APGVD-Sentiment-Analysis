{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4"
   },
   "source": [
    "<a id=\"topo\"></a>\n",
    "___\n",
    "<img src=\"../_images/logo_mei.jpg\" alt=\"Mestrado em Internet das Coisas @ IPT\" width=\"200\"/>\n",
    "<div class=\"alert alert-block alert-success\" align=\"center\">\n",
    "<h1>Análise e Processamento de Grandes Volumes de Dados</h1>\n",
    "<h3>Sentiment analysis - 4ª parte</div>\n",
    "<center><h5>Criado por: Bruno Bernardo / David Carrilho / Rui Rodrigues</h5></center>\n",
    "___\n",
    "\n",
    "[<img src=\"../_images/download.jpg\" alt=\"Mestrado em Internet das Coisas @ IPT\" width=\"50\"/>](lstm.ipynb)\n",
    "___\n",
    "\n",
    "**Crédito para Peter Nagy February 2017** \n",
    "https://github.com/nagypeterjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39c44f0e-d62c-7e11-a542-4fcd58e21442"
   },
   "source": [
    "[Análise sentimental supervisionada (Naive Bayes)](#super)<br>\n",
    "[Preparação](#preparacao)<br>\n",
    "[Importação de módulos](#import)<br>\n",
    "[Dataset](#dataset)<br>\n",
    "[Importação de dados](#import2)<br>\n",
    "[Limpeza dos dados de treino](#limpeza)<br>\n",
    "[Tokenização e sequênciação](#token)<br>\n",
    "[Criação e parametrização da rede](#criacao)<br>\n",
    "[Separação de dados em treino e teste](#split)<br>\n",
    "[Efectivação do treino](#treino)<br>\n",
    "[Testes](#testes)<br>\n",
    "[Avaliação](#avaliacao)<br>\n",
    "[Conclusão](#conclusao)\n",
    "[Referências](#referencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<a id=\"super\"></a>\n",
    "# Análise sentimental supervisionada (LSTM)\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "Agora que temos informação de como se comporta um dos métodos de classificação, em que reconhecemos pontos fortes e pontos fracos vamos abordar o mesmo <i>dataset</i> usando uma rede neuronal recorrente. As redes [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), Long short-term memory são redes que consomem sequências de dados, com memória mas com a capacidade de esquecer. Acreditamos que esta característica poderá superar os pontos fracos do classificador Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparacao\"></a>\n",
    "## Preparação\n",
    "\n",
    "Se correr o notebook no seu próprio server Jupyter deverá instalar dependências.\n",
    "\n",
    "Na \"Anaconda Comand Prompt\" (na pasta onde tens os notebooks):\n",
    "\n",
    "    pip install numpy\n",
    "\n",
    "    pip install pandas\n",
    "\n",
    "    pip install nltk\n",
    "\n",
    "    pip install tensorflow\n",
    "    \n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">Para quem tiver uma gráfica das mais recentes gerações sugere-se a instalação do \"tensorflow-gpu\"</div>\n",
    "<div class=\"alert alert-block alert-info\">NOTA: Se não tiveres premissões para instalar, executa o Prompt como Administrador. Se estiveres da drive C: e os notebooks estiverem na D:, assim que abres o Prompt deves escrever D: para mudar de drive.</div>\n",
    "\n",
    "No nosso caso vamos usar um contentor dinâmico que permite a partilha de notebooks com edição, [mybinder.org](https://mybinder.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "### A importação dos módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# algebra linear\n",
    "import numpy as np \n",
    "# processamento de dados e I/O ficheiros \n",
    "import pandas as pd \n",
    "# Biblioteca de vectorização\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Biblioteca de tokenização\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e"
   },
   "source": [
    "<a id=\"dataset\"></a>\n",
    "### O Dataset\n",
    "\n",
    "Para um <i>benchmark</i> fazer sentido devemos usar os mesmo dados. Se bem que não controlamos exactamente que parcelas são usadas para treino e teste acreditamos poder aferir se há melhorias entre um método de classificação e o outro. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import2\"></a>\n",
    "### Importação dos dados\n",
    "\n",
    "Os nossos <i>dataset</i> são CSVs que contém diversa informação, como o nosso objectivo é fazer análise sentimental precisamos apenas das colunas com o texto do tweet e a classificação atribuida por utilizadores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('../_datasets/GOP_REL_ONLY.csv', encoding = \"ISO-8859-1\")\n",
    "data = pd.read_csv('../_datasets/1377191648_sentiment_nuclear_power.csv', encoding = \"ISO-8859-1\")\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929"
   },
   "source": [
    "<a id=\"limpeza\"></a>\n",
    "### Limpeza dos dados\n",
    "\n",
    "Seguidamente vamos padronizar os dados, remover alguma informação desnecessária para garantir que ficamos apenas com palavras e, como fizémos com o classificador anterior, vamos excluir os <i>tweets</i> neutros pois pretendemos obter diferenciação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data[ data['sentiment'] == 'Positive'].size)\n",
    "print(data[ data['sentiment'] == 'Negative'].size)\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"token\"></a>\n",
    "### Tokenização e sequênciação\n",
    "\n",
    "Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452"
   },
   "source": [
    "<a id=\"criacao\"></a>\n",
    "### Criação e parametrização da rede\n",
    "\n",
    "Agora vamos criar a rede the LSTM. \n",
    "\n",
    "A configuração de parâmetros como **embed_dim**, **lstm_out**, **batch_size**, **droupout** e **rate** requerem experimentação até encontrar os melhores resultados. \n",
    "\n",
    "Vamos usar a função \"[softmax](https://en.wikipedia.org/wiki/Softmax_function)\" para activação do modelo pois num rede de [entropia cruzada](https://en.wikipedia.org/wiki/Cross_entropy) essa é a abordagem correcta.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">A função **softmax** comprime um vector de qualquer dimensão com quaisquer números reais para o intervalo [0,1]</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">**Entropia cruzada** entre duas distruições permite estimar o tamanho de informação necessária para identificar a que distribuição pertence um elemento</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(rate=0.2))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f"
   },
   "source": [
    "<a id=\"split\"></a>\n",
    "### Separação de dados em treino e teste\n",
    "\n",
    "Poderemos \"brincar\" com a proporção para perceber como influencia os resultados.\n",
    "\n",
    "**random_state** define a aleatoridade com que a rede se lembra e esquece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6"
   },
   "source": [
    "<a id=\"treino\"></a>\n",
    "### Efectivação do treino\n",
    "\n",
    "Agora resta-nos efectuar o treino. Idealmente deveríamos de usar um elevado número de <i>epochs</i> mas em nome da fluidez da explicação vamos restringir-nos a 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 9, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434"
   },
   "source": [
    "<a id=\"teste\"></a>\n",
    "## Agora um teste\n",
    "\n",
    "Vamos pegar em alguns dados, testá-los e medir verificar um par de indicadores de performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"Indicadores de performance:\")\n",
    "print(\"score: %.2f%%\" % (score*100))\n",
    "print(\"acc: %.2f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf"
   },
   "source": [
    "<a id=\"avaliacao\"></a>\n",
    "### Avaliação de resultados\n",
    "\n",
    "Vamos medir o número de acertos e reflectir sobre eles e posteriormente compará-los com o outro classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1add73e9-c6fb-7e4c-8715-ea92f519d2a6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    \n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "\n",
    "print('[Negative]: %0.2f%%'  % (neg_correct/neg_cnt*100))       \n",
    "print('[Positive]: %0.2f%%'  % (pos_correct/pos_cnt*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusao\"></a>\n",
    "## Conclusão\n",
    "\n",
    "Pelos resultados obtidos torna-se claro que ambos os métodos são muito bons a encontraar tweets negativos mas no momento da decisão se são positivos algo falha. Podemos especular que são os próprios <i>datasets</i> que têm muito mais <i>tweets</> negativos que positivos. Para isso temos outro <i>dataset</i>. Havendo tempo podemos efectuar alguma experiências.\n",
    "\n",
    "E sobre análise de sentimentos? Estes exercícios mais técnicos pretendem demonstrar que os resultados de uma análise não dependem exclusivamente do peso ou conotação das palavras mas também dos métodos e parametros usados nos classificadores, da precisão da categorização dos dados de treino, da vizinhança das palavras etc. \n",
    "\n",
    "Um dos métodos que gostaríamos de ter explorado baseia-se em florestas aleatórias de decisão, que afectam consideravelmente o a conotação de uma palavra com as suas vizinhas (como podémos testar nos 2 primeiros exercícios). No entanto, apesar de termos feito investigação e experiências, esse <i>notebook</i> seria já muito pesado e faria-nos exceder os objectivos da unidade curricular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
    "collapsed": true
   },
   "source": [
    "[<div align=\"right\" class=\"alert alert-block\"><img src=\"../_images/top.png\" width=\"20\"></div>](#topo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "<a id=\"referencias\"></a>\n",
    "\n",
    "# Referências\n",
    "\n",
    "http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D<br>\n",
    "https://en.wikipedia.org/wiki/Cross_entropy<br>\n",
    "https://en.wikipedia.org/wiki/Softmax_function"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 185,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
